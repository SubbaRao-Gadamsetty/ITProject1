# Quick Start: 5-Minute Real-Time Setup

This is the TL;DR version to get up and running in 5 minutes.

## What You'll Have After This

- âœ… Prometheus running locally (scrapes metrics)
- âœ… Alertmanager running locally (detects anomalies)
- âœ… Grafana running locally (visualizes data)
- âœ… Fake metrics generator (creates issues on demand)
- âœ… Your incident response system connected via webhook
- âœ… **Fully automated incident detection and resolution**

## Step 1: Copy Configuration Files (1 minute)

Create these directories and files in your project root:

### Create `monitoring/` directory

```bash
mkdir monitoring
```

### Save this as `monitoring/prometheus.yml`

```yaml
global:
  scrape_interval: 10s
  evaluation_interval: 10s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']

rule_files:
  - '/etc/prometheus/rules.yml'

scrape_configs:
  - job_name: 'database-server'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'

  - job_name: 'app-server'
    static_configs:
      - targets: ['localhost:8001']
    metrics_path: '/metrics'

  - job_name: 'api-gateway'
    static_configs:
      - targets: ['localhost:8002']
    metrics_path: '/metrics'
```

### Save this as `monitoring/rules.yml`

```yaml
groups:
  - name: incident_rules
    interval: 10s
    rules:
      - alert: DatabaseConnectionTimeout
        expr: rate(db_connection_timeout_total[1m]) > 0.5
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database timeouts detected"
          description: "{{ $labels.instance }} database timeouts"

      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes > 500000000
        for: 1m
        labels:
          severity: warning
          component: memory
        annotations:
          summary: "High memory usage"
          description: "Memory > 500MB on {{ $labels.instance }}"

      - alert: APIGateway503
        expr: rate(http_requests_total{status="503"}[1m]) > 0.1
        for: 1m
        labels:
          severity: critical
          component: api_gateway
        annotations:
          summary: "API Gateway 503 errors"
          description: "High 503 error rate on {{ $labels.instance }}"
```

### Save this as `monitoring/alertmanager.yml`

```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'incident-response'
  group_by: ['alertname']
  group_wait: 10s

receivers:
  - name: 'incident-response'
    webhook_configs:
      - url: 'http://localhost:5000/webhooks/alertmanager'
        send_resolved: true
```

### Save this as `docker-compose.yml` (in project root)

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/rules.yml:/etc/prometheus/rules.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - monitoring

networks:
  monitoring:
    driver: bridge
```

## Step 2: Start the Stack (1 minute)

```bash
# Start Prometheus, Alertmanager, Grafana
docker-compose up -d

# Check status
docker-compose ps

# You should see 3 services running
```

## Step 3: Run Metrics Generator (1 minute)

First, install the dependency:
```bash
pip install prometheus-client
```

Create `monitoring/simple_metrics_exporter.py`:

```python
from prometheus_client import Counter, Gauge, start_http_server
import time
import random
from flask import Flask, request
import threading

# Metrics
db_timeout = Counter('db_connection_timeout_total', 'DB timeouts', ['instance'])
memory = Gauge('process_resident_memory_bytes', 'Memory', ['instance'])
http_503 = Counter('http_requests_total', 'HTTP requests', ['instance', 'status'])

app = Flask(__name__)
current_scenario = None

@app.route('/metrics')
def metrics():
    # Auto-generated by prometheus_client
    pass

@app.route('/trigger/<scenario>', methods=['POST'])
def trigger(scenario):
    global current_scenario
    current_scenario = scenario
    return {"status": f"Started {scenario}"}, 200

def update_metrics():
    while True:
        if current_scenario == "database_timeout":
            db_timeout.labels(instance='db-server-01:8000').inc(random.uniform(2, 5))
        if current_scenario == "memory_leak":
            memory.labels(instance='app-server-01:8001').set(random.uniform(500000000, 900000000))
        if current_scenario == "api_503":
            http_503.labels(instance='api-gateway-01:8002', status='503').inc(random.randint(5, 15))
        time.sleep(5)

if __name__ == '__main__':
    start_http_server(8000)
    threading.Thread(target=update_metrics, daemon=True).start()
    app.run(port=5001)
```

Run it:
```bash
python monitoring/simple_metrics_exporter.py
```

## Step 4: Connect Your System (1 minute)

Add this to your `it_incident_response/system.py`:

```python
from flask import Flask, request, jsonify
import threading

def add_webhook_receiver(system):
    """Add Alertmanager webhook receiver to system"""
    
    app = Flask(__name__)
    
    @app.route('/webhooks/alertmanager', methods=['POST'])
    def handle_alert():
        payload = request.json
        for alert in payload.get('alerts', []):
            if alert['status'] == 'firing':
                # Get alert info
                alert_name = alert['labels'].get('alertname')
                instance = alert['labels'].get('instance', 'unknown')
                
                # Map to incident
                incident_map = {
                    'DatabaseConnectionTimeout': {
                        'title': f'Database Connection Timeout',
                        'description': f'Detected on {instance}',
                        'severity': 'high',
                        'affected_systems': [instance.split(':')[0]],
                        'tags': ['database', 'connectivity']
                    },
                    'HighMemoryUsage': {
                        'title': f'High Memory Usage',
                        'description': f'Memory spike on {instance}',
                        'severity': 'warning',
                        'affected_systems': [instance.split(':')[0]],
                        'tags': ['memory']
                    },
                    'APIGateway503': {
                        'title': f'API Gateway 503 Errors',
                        'description': f'503 errors from {instance}',
                        'severity': 'critical',
                        'affected_systems': [instance.split(':')[0]],
                        'tags': ['api', '503']
                    }
                }
                
                incident_data = incident_map.get(alert_name, {
                    'title': alert_name,
                    'description': f'Alert from {instance}',
                    'severity': 'medium',
                    'affected_systems': [instance.split(':')[0]],
                    'tags': []
                })
                
                # Create incident
                incident_id = system.create_incident(**incident_data)
                print(f"âœ… Incident created: {incident_id}")
                
                # Auto-analyze
                system.analyze_incident(incident_id)
                
                # Auto-resolve
                system.implement_resolution(incident_id)
                
                return {"status": "ok"}, 200
        
        return {"status": "ok"}, 200
    
    # Start webhook in background thread
    def run_webhook():
        app.run(port=5000, debug=False)
    
    webhook_thread = threading.Thread(target=run_webhook, daemon=True)
    webhook_thread.start()
```

## Step 5: Test It (1 minute)

```bash
# In one terminal, start your system
python run_demo.py

# In another terminal, trigger an incident
curl -X POST http://localhost:5001/trigger/database_timeout

# Watch what happens:
# 1. Metrics generator starts generating timeout errors
# 2. Prometheus evaluates (wait 1-2 minutes)
# 3. Alert fires
# 4. Alertmanager sends webhook
# 5. Your system creates incident
# 6. Incident auto-analyzed
# 7. Incident auto-resolved
```

## Verify It's Working

```bash
# Check Prometheus has data
curl http://localhost:9090/api/v1/query?query=db_connection_timeout_total

# Check Alertmanager
curl http://localhost:9093/api/v1/alerts

# Check Grafana
# Visit http://localhost:3000
# Login: admin/admin
```

## That's It! ðŸŽ‰

You now have:
- âœ… Real Prometheus metrics
- âœ… Real Alertmanager alerts
- âœ… Real-time incident detection
- âœ… Automatic incident resolution
- âœ… All triggered from code

## What's Happening Behind the Scenes

```
You trigger incident
    â†“
Metrics exporter generates errors
    â†“
Prometheus scrapes every 10s
    â†“
Alert rule evaluates
    â†“
Alertmanager fires alert (after 1m)
    â†“
Webhook calls your system
    â†“
Incident created automatically
    â†“
Diagnostic agent analyzes
    â†“
Resolution agent fixes it
    â†“
JIRA ticket updated (if configured)
    â†“
Incident closed
```

**Total time: 2-3 minutes (all automatic)**

## Next: Enhance Your System

To make it production-grade:

1. Add approval gates before executing fixes
2. Implement dry-run mode to preview changes
3. Connect to real Prometheus/Alertmanager
4. Add more alert rules
5. Integrate with Slack for notifications
6. Add historical incident tracking

See `REAL_TIME_INTEGRATION_GUIDE.md` for full implementation details.
